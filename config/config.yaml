repo:
  path: "${REPO_PATH}"
  dev_branch: "${DEV_BRANCH}"
  default_remote: origin

# Server configuration
server:
  port: 8001  # Changed from 8000 to avoid conflicts with other repos
  host: "0.0.0.0"

# Local-first by default to honor your preconfigured CLIs.
providers:
  order:
    - "claude_cli"
    - "codex_cli"
    - "gemini_cli"
    - "cursor_cli"
    - "anthropic_api"
    - "openai_api"
    - "gemini_api"

  # Full access mode providers (interactive/session mode)
  full_access_order:
    - "claude_interactive"
    - "codex_interactive"
    - "claude_cli"
    - "anthropic_api"

  claude_cli:
    mode: "cli"
    provider_type: "cli"
    binary: "claude"             # Anthropic Claude Code CLI. Non-interactive via -p.
    args: ["-p", "--output-format", "text"]  # or "json" if you prefer structured
    model: "sonnet"              # optional: e.g., "claude-sonnet-4-20250514"
    available_models: ["default", "sonnet", "opus", "haiku"]
    description: "Claude Code CLI - Local Claude tool with file system access"

  codex_cli:
    mode: "cli"
    provider_type: "cli"
    binary: "codex"              # OpenAI Codex CLI (terminal agent). Non-interactive via 'exec'.
    args: ["exec"]               # will append the prompt at the end
    model: "o4-mini"             # CLI respects config; this is just a hint to include in prompts
    available_models: ["default", "o4-mini", "o3", "gpt-5", "gpt-4.1"]
    description: "Codex CLI - OpenAI terminal agent for code generation and execution"

  gemini_cli:
    mode: "cli"
    provider_type: "cli"
    binary: "gemini"             # Google Gemini CLI. Non-interactive via -p
    args: ["-p"]                 # one-shot prompt mode
    model: "gemini-2.5-pro"      # or gemini-2.5-flash if you want speed
    available_models: ["default", "gemini-2.5-pro", "gemini-2.5-flash"]
    description: "Gemini CLI - Google's multimodal AI with fast execution"

  cursor_cli:
    mode: "cli" 
    provider_type: "cli"
    binary: "cursor-agent"       # Cursor CLI. Non-interactive via -p
    args: ["-p", "--output-format", "text"]
    available_models: ["default"]
    description: "Cursor CLI - AI code editor optimized for refactoring"

  anthropic_api:
    mode: "api"
    provider_type: "api"
    model: "claude-opus-4-1-20250805"
    available_models: ["claude-opus-4-1-20250805", "claude-sonnet-4", "claude-opus-4", "claude-3-5-sonnet-latest", "claude-3-5-sonnet-20241022", "claude-3-5-haiku-latest", "claude-3-5-haiku-20241022"]
    max_tokens: 4096
    description: "Anthropic API - Official Claude API with latest models including Opus 4.1 and Sonnet 4"

  openai_api:
    mode: "api"
    provider_type: "api"
    model: "gpt-5"
    available_models: ["gpt-5", "gpt-5-mini", "gpt-5-nano", "o3", "o4-mini", "gpt-4.1", "gpt-4o", "gpt-4o-mini"]
    description: "OpenAI API - GPT models including GPT-5, o3, and o4-mini with reliable API access"

  gemini_api:
    mode: "api"
    provider_type: "api"
    model: "gemini-2.0-flash-exp"
    available_models: ["gemini-2.0-flash-exp", "gemini-1.5-pro-latest", "gemini-1.5-pro", "gemini-1.5-flash-latest", "gemini-1.5-flash", "gemini-1.0-pro"]
    description: "Google Gemini API - Multimodal AI with fast processing"

  # Interactive/Full Access Mode Providers
  claude_interactive:
    mode: "interactive"
    provider_type: "cli"
    binary: "claude"
    args: ["--dangerously-skip-permissions"]  # Full access mode bypassing permissions
    model: "sonnet"
    available_models: ["default", "sonnet", "opus", "haiku"]
    description: "Claude Code in full access mode with permissions bypassed"

  codex_interactive:
    mode: "interactive"
    provider_type: "cli"
    binary: "codex"
    args: ["--ask-for-approval", "never", "--sandbox", "danger-full-access"]
    model: "o4-mini"
    available_models: ["default", "o4-mini", "o3", "gpt-5", "gpt-4.1"]
    description: "Codex in full access mode with auto-approval and full sandbox access"

# Full access mode configuration
full_access:
  enabled: true
  default_provider_order: "full_access_order"
  repo_context: true  # Include full repository context
  allow_file_operations: true  # Enable create/read/write/delete operations
  allow_system_commands: true  # Enable system command execution

hygiene:
  checkpoint_minutes: 30
  pr_minutes: 120
  conventional_commits: true

worktrees:
  base_dir: "./worktrees"

# Multi-PR merge conflict resolution strategy
merge_strategy:
  mode: "sequential_rebase"  # Sequential processing with auto-rebase
  auto_rebase: true          # Automatically rebase branches before merging
  conflict_resolution: "auto" # auto | escalate | manual
  merge_order: ["security", "bug", "feature", "docs"]  # Priority order
  process_interval_minutes: 5  # How often to process merge queue
  max_concurrent_rebases: 3    # Limit concurrent rebase operations
  
# Merge queue configuration  
merge_queue:
  enabled: true
  auto_process: true         # Automatically process queue
  notify_conflicts: true     # Send notifications for unresolved conflicts
  escalation_timeout: 60     # Minutes before escalating conflicts

# Built-in roles (you can delete if you move them to roles/*.yaml)
roles:
  backend:
    branch_prefix: "feat/backend"
    reviewers: ["@org/backend-team"]
    prompt: |
      You are a pragmatic backend engineer. Follow FastAPI+pytest+ruff+mypy conventions.
  frontend:
    branch_prefix: "feat/frontend"
    reviewers: ["@org/frontend-team"]
    prompt: |
      You are a product-minded frontend engineer. Use modern React, Vite, and TypeScript.
  data:
    branch_prefix: "feat/data"
    reviewers: ["@org/data-team"]
    prompt: |
      You are a data engineer. Prefer Polars over pandas; write modular ETL and unit tests.
  ml:
    branch_prefix: "feat/ml"
    reviewers: ["@org/ml-team"]
    prompt: |
      You are an ML engineer. Add tests, deterministic seeds, and metrics logging.

# Directory where extra role YAMLs live (auto-loaded)
roles_dir: "roles"